{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "city-circuits: prep wikipedia cities dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UTYt2F1UDgpl"
      ],
      "mount_file_id": "1sIPNjncldPIPfV6hobTWPKXBm4Ms4T9c",
      "authorship_tag": "ABX9TyNVyIIMzjxOZ3WDZZ0KcJXo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmerizia/city-circuits/blob/main/city_circuits_prep_wikipedia_cities_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTYt2F1UDgpl"
      },
      "source": [
        "# define encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv2BI2DxDVgU"
      },
      "source": [
        "'''\n",
        "Modified from https://github.com/graykode/gpt-2-Pytorch/blob/master/GPT2/encoder.py\n",
        "See above for original license.\n",
        "'''\n",
        "\n",
        "\"\"\"Byte pair encoding utilities\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import regex as re\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
        "        self.errors = errors # how to handle errors in decoding\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
        "        self.cache = {}\n",
        "\n",
        "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
        "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
        "        return text\n",
        "\n",
        "def get_encoder():\n",
        "    with open('./encoder.json', 'r') as f:\n",
        "        encoder = json.load(f)\n",
        "    with open('./vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
        "    return Encoder(\n",
        "        encoder=encoder,\n",
        "        bpe_merges=bpe_merges,\n",
        "    )"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmvhfoczEEZi",
        "outputId": "240760c8-1ba2-41f0-e17a-a8b108b1bde2"
      },
      "source": [
        "!curl -o encoder.json https://raw.githubusercontent.com/graykode/gpt-2-Pytorch/master/GPT2/encoder.json\n",
        "!curl -o vocab.bpe https://raw.githubusercontent.com/graykode/gpt-2-Pytorch/master/GPT2/vocab.bpe"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1017k  100 1017k    0     0  3497k      0 --:--:-- --:--:-- --:--:-- 3497k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  445k  100  445k    0     0  1912k      0 --:--:-- --:--:-- --:--:-- 1912k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_9W12yiDszX"
      },
      "source": [
        "# build the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73jHzHQlE5LI",
        "outputId": "43d0b60b-6c20-4ce4-8bff-7205b2e84c8a"
      },
      "source": [
        "import requests\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "city_list_urls = [\n",
        "    'https://en.wikipedia.org/wiki/List_of_towns_and_cities_with_100,000_or_more_inhabitants/country:_A-B',\n",
        "    'https://en.wikipedia.org/wiki/List_of_towns_and_cities_with_100,000_or_more_inhabitants/country:_C-D-E-F',\n",
        "    'https://en.wikipedia.org/wiki/List_of_towns_and_cities_with_100,000_or_more_inhabitants/country:_G-H-I-J-K',\n",
        "    'https://en.wikipedia.org/wiki/List_of_towns_and_cities_with_100,000_or_more_inhabitants/country:_L-M-N-O',\n",
        "    'https://en.wikipedia.org/wiki/List_of_towns_and_cities_with_100,000_or_more_inhabitants/country:_P-Q-R-S',\n",
        "    'https://en.wikipedia.org/wiki/List_of_towns_and_cities_with_100,000_or_more_inhabitants/country:_T-U-V-W-Y-Z',\n",
        "]\n",
        "\n",
        "links = []\n",
        "for url in city_list_urls:\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    content_divs = soup.find_all('div', { 'class': 'mw-parser-output' } )\n",
        "    assert len(content_divs) == 1\n",
        "    content_div = content_divs[0]\n",
        "    for table in content_div.find_all('table'):\n",
        "        if 'box-More_citations_needed' in table['class'] or 'box-Cleanup' in table['class']:\n",
        "            continue\n",
        "        for tr in table.find_all('tr')[1:]:\n",
        "            td = tr.find('td')\n",
        "            if td:\n",
        "                city_page_url = td.find('a')['href']\n",
        "                links.append(urllib.parse.unquote(city_page_url))\n",
        "    # some lists are in <ul> or <ol> tags\n",
        "    cnt = 0\n",
        "    for ul in content_div.find_all('ul'):\n",
        "        for li in ul.find_all('li'):\n",
        "            a = li.find('a')\n",
        "            if a:\n",
        "                link_text = a.text\n",
        "                link_url = a['href']\n",
        "                if link_url.startswith('/wiki/') and link_text != 'World largest cities':\n",
        "                    city_page_url = urllib.parse.unquote(link_url)\n",
        "                    # print(city_page_url)\n",
        "                    links.append(city_page_url)\n",
        "                    cnt += 1\n",
        "    cnt = 0\n",
        "    for ol in content_div.find_all('ol'):\n",
        "        for li in ol.find_all('li'):\n",
        "            a = li.find('a')\n",
        "            if a:\n",
        "                link_text = a.text\n",
        "                link_url = a['href']\n",
        "                if link_url.startswith('/wiki/') and link_text != 'World largest cities':\n",
        "                    city_page_url = urllib.parse.unquote(link_url)\n",
        "                    # print(city_page_url)\n",
        "                    links.append(city_page_url)\n",
        "                    cnt += 1\n",
        "\n",
        "print('Collected', len(links), 'links to cities from various lists on Wikipedia.')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 4394 links to cities from various lists on Wikipedia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTKC2l57jAat"
      },
      "source": [
        "import time\n",
        "import requests\n",
        "\n",
        "if 'data' in globals():\n",
        "    del data\n",
        "\n",
        "def download(url):\n",
        "    for _ in range(2):\n",
        "        r = requests.get(url)\n",
        "        if r.status_code == 200:\n",
        "            return r\n",
        "        print(f'failure for url {url}! trying again...')\n",
        "\n",
        "urls = ['https://wikipedia.com' + urllib.parse.quote(link) for link in links]\n",
        "\n",
        "st = time.time()\n",
        "data = []\n",
        "for idx, url in enumerate(urls):\n",
        "    r = download(url)\n",
        "    data.append(r)\n",
        "    print(idx, '/', len(urls), r.status_code)\n",
        "en = time.time()\n",
        "print(en-st)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iutnp8KVV0Ry"
      },
      "source": [
        "def fix_typos(text, page_name):\n",
        "    # fix typos in Wikipedia (we should fix these in Wikipedia shortly)\n",
        "    if page_name == 'Djelfa':\n",
        "        # missing paren\n",
        "        text = text.replace('(Arabic: الجلفة\\u200e, romanized:\\xa0al-Ǧilfah ', '')\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    original = text\n",
        "\n",
        "    # we probably won't need more than 1k characters\n",
        "    text = text[:1000]\n",
        "\n",
        "    # remove parenthesized portions\n",
        "    k1 = 0\n",
        "    k2 = 0\n",
        "    k3 = 0\n",
        "    new_text = ''\n",
        "    for i in range(len(text)):\n",
        "        if text[i] == '(':\n",
        "            k1 += 1\n",
        "        elif text[i] == ')':\n",
        "            k1 -= 1\n",
        "        elif text[i] == '[':\n",
        "            k2 += 1\n",
        "        elif text[i] == ']':\n",
        "            k2 -= 1\n",
        "        elif text[i] == '{':\n",
        "            k3 += 1\n",
        "        elif text[i] == '}':\n",
        "            k3 -= 1\n",
        "        else:\n",
        "            if k1 == 0 and k2 == 0 and k3 == 0:\n",
        "                new_text += text[i]\n",
        "    text = new_text\n",
        "\n",
        "    # fix strange punctuation\n",
        "    text = text.replace(' .', '.')\n",
        "    text = text.replace(' ,', ',')\n",
        "    text = text.replace(' ; ', '')\n",
        "\n",
        "    # put everything on one line\n",
        "    text = ' '.join(text.split('\\n'))\n",
        "\n",
        "    # clean up white space\n",
        "    text = ' '.join(text.split()).strip()\n",
        "\n",
        "    # possible degenerate cases\n",
        "    if len(text) < 5:\n",
        "        return\n",
        "\n",
        "    # only take the first few tokens\n",
        "    num_tokens = 30\n",
        "    text = text[:num_tokens * 10]\n",
        "    tokens = enc.encode(text)\n",
        "    tokens = tokens[:num_tokens]\n",
        "    text = enc.decode(tokens)\n",
        "    return text\n",
        "\n",
        "\n",
        "results = []\n",
        "for idx, request in enumerate(data):\n",
        "    # if idx != 1294:\n",
        "    #     continue\n",
        "    # print(request.url)\n",
        "    soup = BeautifulSoup(request.text, 'html.parser')\n",
        "    content_divs = soup.find_all('div', { 'class': 'mw-parser-output' } )\n",
        "    assert len(content_divs) == 1\n",
        "    content_div = content_divs[0]\n",
        "    # remove coodinates section\n",
        "    for span in content_div.find_all('span'):\n",
        "        if span.get('id') == 'coordinates':\n",
        "            span.extract()\n",
        "    paragraphs = content_div.find_all('p')\n",
        "    text = ''\n",
        "    for paragraph in paragraphs:\n",
        "        line = paragraph.text.strip()\n",
        "        if len(line) > 0:\n",
        "            text += ' ' + line\n",
        "    # print(text)\n",
        "    text = fix_typos(text, urllib.parse.unquote(request.url).split('/')[-1])\n",
        "    text = clean_text(text)\n",
        "    results.append({ 'url': request.url, 'text': text })\n",
        "    print(idx, '/', len(data), request.url, ':', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoh02lT4_MQH"
      },
      "source": [
        "# save the results to disk\n",
        "import json\n",
        "with open('dataset.json', 'w') as f:\n",
        "    f.write(json.dumps(results))"
      ],
      "execution_count": 236,
      "outputs": []
    }
  ]
}