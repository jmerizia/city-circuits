{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2-neurons-faster.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "z-WLzinyDP8W",
        "5gDVVb9IDVkC",
        "zsGTQhmlDeYT"
      ],
      "authorship_tag": "ABX9TyOBCYbk2pKP212Bq4ZMV6P+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmerizia/city-circuits/blob/main/gpt2_neurons_faster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-WLzinyDP8W"
      },
      "source": [
        "# licenses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeJWZjigDPDs"
      },
      "source": [
        "# For this notebook\n",
        "\n",
        "# MIT License\n",
        "\n",
        "# Copyright (c) 2021 Jacob Merizian\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in all\n",
        "# copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "# SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qZko39-DUyJ"
      },
      "source": [
        "# source: https://github.com/graykode/gpt-2-Pytorch/blob/master/LICENSE\n",
        "\n",
        "# MIT License\n",
        "\n",
        "# Copyright (c) 2019 OpenAI, HugginFace Inc. team. and TaeHwan Jung\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in all\n",
        "# copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "# SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbcTHGVHXGoE"
      },
      "source": [
        "# source: https://github.com/openai/gpt-2/blob/master/LICENSE\n",
        "\n",
        "# Modified MIT License\n",
        "\n",
        "# Software Copyright (c) 2019 OpenAI\n",
        "\n",
        "# We don’t claim ownership of the content you create with GPT-2, so it is yours to do with as you please.\n",
        "# We only ask that you use GPT-2 responsibly and clearly indicate your content was created using GPT-2.\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and\n",
        "# associated documentation files (the \"Software\"), to deal in the Software without restriction,\n",
        "# including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so,\n",
        "# subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included\n",
        "# in all copies or substantial portions of the Software.\n",
        "# The above copyright notice and this permission notice need not be included\n",
        "# with content created by the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n",
        "# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n",
        "# BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n",
        "# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE\n",
        "# OR OTHER DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zssfVTmCXS1-"
      },
      "source": [
        "# Ensure you are using GPU acceleration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzOMD0UhDFzq"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gDVVb9IDVkC"
      },
      "source": [
        "# define encoder and other utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRqsZhn1DVOs"
      },
      "source": [
        "'''\n",
        "From https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
        "'''\n",
        "\n",
        "\"\"\"Byte pair encoding utilities\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import regex as re\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
        "        self.errors = errors # how to handle errors in decoding\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
        "        self.cache = {}\n",
        "\n",
        "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
        "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
        "        return text\n",
        "\n",
        "def get_encoder():\n",
        "    with open('./encoder.json', 'r') as f:\n",
        "        encoder = json.load(f)\n",
        "    with open('./vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
        "    return Encoder(\n",
        "        encoder=encoder,\n",
        "        bpe_merges=bpe_merges,\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD1nAKepDcrp"
      },
      "source": [
        "'''\n",
        "Modified from https://github.com/graykode/gpt-2-Pytorch/blob/master/GPT2/utils.py\n",
        "See above for original license.\n",
        "'''\n",
        "\n",
        "'''\n",
        "    code by TaeHwan Jung(@graykode)\n",
        "    Original Paper and repository here : https://github.com/openai/gpt-2\n",
        "    GPT2 Pytorch Model : https://github.com/huggingface/pytorch-pretrained-BERT\n",
        "'''\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def load_weight(model, state_dict):\n",
        "    old_keys = []\n",
        "    new_keys = []\n",
        "    for key in state_dict.keys():\n",
        "        new_key = None\n",
        "        if key.endswith(\".g\"):\n",
        "            new_key = key[:-2] + \".weight\"\n",
        "        elif key.endswith(\".b\"):\n",
        "            new_key = key[:-2] + \".bias\"\n",
        "        elif key.endswith(\".w\"):\n",
        "            new_key = key[:-2] + \".weight\"\n",
        "        if new_key:\n",
        "            old_keys.append(key)\n",
        "            new_keys.append(new_key)\n",
        "    for old_key, new_key in zip(old_keys, new_keys):\n",
        "        state_dict[new_key] = state_dict.pop(old_key)\n",
        "\n",
        "    missing_keys = []\n",
        "    unexpected_keys = []\n",
        "    error_msgs = []\n",
        "    # copy state_dict so _load_from_state_dict can modify it\n",
        "    metadata = getattr(state_dict, \"_metadata\", None)\n",
        "    state_dict = state_dict.copy()\n",
        "    if metadata is not None:\n",
        "        state_dict._metadata = metadata\n",
        "\n",
        "    def load(module, prefix=\"\"):\n",
        "        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
        "        module._load_from_state_dict(\n",
        "            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs\n",
        "        )\n",
        "        for name, child in module._modules.items():\n",
        "            if child is not None:\n",
        "                load(child, prefix + name + \".\")\n",
        "\n",
        "    start_model = model\n",
        "    if hasattr(model, \"transformer\") and all(not s.startswith('transformer.') for s in state_dict.keys()):\n",
        "        start_model = model.transformer\n",
        "    load(start_model, prefix=\"\")\n",
        "\n",
        "    # Make sure we are still sharing the output and input embeddings after loading weights\n",
        "    model.set_tied()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCbHrh5fDthr"
      },
      "source": [
        "!curl -o encoder.json https://raw.githubusercontent.com/jmerizia/city-circuits/main/encoder.json\n",
        "!curl -o vocab.bpe https://raw.githubusercontent.com/jmerizia/city-circuits/main/vocab.bpe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsGTQhmlDeYT"
      },
      "source": [
        "# gpt model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HGoWz14DdH9"
      },
      "source": [
        "'''\n",
        "Modified from https://github.com/graykode/gpt-2-Pytorch/blob/master/GPT2/config.py\n",
        "See above for original license.\n",
        "'''\n",
        "\n",
        "'''\n",
        "    code by TaeHwan Jung(@graykode)\n",
        "    Original Paper and repository here : https://github.com/openai/gpt-2\n",
        "    GPT2 Pytorch Model : https://github.com/huggingface/pytorch-pretrained-BERT\n",
        "'''\n",
        "class GPT2Config(object):\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size_or_config_json_file=50257,\n",
        "            n_positions=1024,\n",
        "            n_ctx=1024,\n",
        "            n_embd=1600,\n",
        "            n_layer=48,\n",
        "            n_head=25,\n",
        "            layer_norm_epsilon=1e-5,\n",
        "            initializer_range=0.02,\n",
        "    ):\n",
        "        self.vocab_size = vocab_size_or_config_json_file\n",
        "        self.n_ctx = n_ctx\n",
        "        self.n_positions = n_positions\n",
        "        self.n_embd = n_embd\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.layer_norm_epsilon = layer_norm_epsilon\n",
        "        self.initializer_range = initializer_range\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tkiTXp2Dh8E"
      },
      "source": [
        "'''\n",
        "Modified from https://github.com/graykode/gpt-2-Pytorch/blob/master/GPT2/model.py\n",
        "See above for original license.\n",
        "'''\n",
        "\n",
        "'''\n",
        "    code by TaeHwan Jung(@graykode)\n",
        "    Original Paper and repository here : https://github.com/openai/gpt-2\n",
        "    GPT2 Pytorch Model : https://github.com/huggingface/pytorch-pretrained-BERT\n",
        "'''\n",
        "import copy\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "ACTIVATION_THRESHOLD = 2\n",
        "\n",
        "def neuron_name_matches_specifier(name, specifier):\n",
        "    for n, s in zip(name.split(':'), specifier.split(':')):\n",
        "        if s != '*' and s != n:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-12):\n",
        "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "        \"\"\"\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.weight * x + self.bias\n",
        "\n",
        "class Conv1D(nn.Module):\n",
        "    def __init__(self, nf, nx):\n",
        "        super(Conv1D, self).__init__()\n",
        "        self.nf = nf\n",
        "        w = torch.empty(nx, nf)\n",
        "        nn.init.normal_(w, std=0.02)\n",
        "        self.weight = Parameter(w)\n",
        "        self.bias = Parameter(torch.zeros(nf))\n",
        "\n",
        "    def forward(self, x):\n",
        "        size_out = x.size()[:-1] + (self.nf,)\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
        "        x = x.view(*size_out)\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, nx, n_ctx, config, scale=False):\n",
        "        super(Attention, self).__init__()\n",
        "        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n",
        "        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n",
        "        assert n_state % config.n_head == 0\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "        self.n_head = config.n_head\n",
        "        self.split_size = n_state\n",
        "        self.scale = scale\n",
        "        self.c_attn = Conv1D(n_state * 3, nx)\n",
        "        self.c_proj = Conv1D(n_state, nx)\n",
        "\n",
        "    def _attn(self, q, k, v):\n",
        "        w = torch.matmul(q, k)\n",
        "        if self.scale:\n",
        "            w = w / math.sqrt(v.size(-1))\n",
        "        nd, ns = w.size(-2), w.size(-1)\n",
        "        b = self.bias[:, :, ns-nd:ns, :ns]\n",
        "        w = w * b - 1e10 * (1 - b)\n",
        "        w = nn.Softmax(dim=-1)(w)\n",
        "        return torch.matmul(w, v)\n",
        "\n",
        "    def merge_heads(self, x):\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
        "        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n",
        "\n",
        "    def split_heads(self, x, k=False):\n",
        "        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
        "        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n",
        "        if k:\n",
        "            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n",
        "        else:\n",
        "            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        x = self.c_attn(x)\n",
        "        query, key, value = x.split(self.split_size, dim=2)\n",
        "        query = self.split_heads(query)\n",
        "        key = self.split_heads(key, k=True)\n",
        "        value = self.split_heads(value)\n",
        "        if layer_past is not None:\n",
        "            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below\n",
        "            key = torch.cat((past_key, key), dim=-1)\n",
        "            value = torch.cat((past_value, value), dim=-2)\n",
        "        present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking\n",
        "        a = self._attn(query, key, value)\n",
        "        a = self.merge_heads(a)\n",
        "        a = self.c_proj(a)\n",
        "        return a, present\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n",
        "        super(MLP, self).__init__()\n",
        "        nx = config.n_embd\n",
        "        self.config = config\n",
        "        self.c_fc = Conv1D(n_state, nx)\n",
        "        self.c_proj = Conv1D(nx, n_state)\n",
        "        self.act = gelu\n",
        "        # self.register_buffer('h', torch.zeros(1, 15, config.n_embd * 4))\n",
        "        # self.register_buffer('h2', torch.zeros(1, 15, config.n_embd))\n",
        "\n",
        "    def forward(self, x, name_prefix=''):\n",
        "        h = self.act(self.c_fc(x))\n",
        "        h2 = self.c_proj(h)\n",
        "        return h2\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_ctx, config, scale=False):\n",
        "        super(Block, self).__init__()\n",
        "        nx = config.n_embd\n",
        "        self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
        "        self.attn = Attention(nx, n_ctx, config, scale)\n",
        "        self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
        "        self.mlp = MLP(4 * nx, config)\n",
        "\n",
        "    def forward(self, x, layer_past=None, name_prefix=''):\n",
        "        a, present = self.attn(self.ln_1(x), layer_past=layer_past)\n",
        "        x = x + a\n",
        "        m = self.mlp(self.ln_2(x), name_prefix=name_prefix + ':mlp')\n",
        "        x = x + m\n",
        "        return x, present\n",
        "\n",
        "class GPT2Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GPT2Model, self).__init__()\n",
        "        self.n_layer = config.n_layer\n",
        "        self.n_embd = config.n_embd\n",
        "        self.n_vocab = config.vocab_size\n",
        "\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
        "        block = Block(config.n_ctx, config, scale=True)\n",
        "        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])\n",
        "        self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "\n",
        "    def set_embeddings_weights(self, model_embeddings_weights):\n",
        "        embed_shape = model_embeddings_weights.shape\n",
        "        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n",
        "        self.decoder.weight = model_embeddings_weights  # Tied weights\n",
        "\n",
        "    def forward(self, input_ids, position_ids=None, token_type_ids=None, past=None):\n",
        "        if past is None:\n",
        "            past_length = 0\n",
        "            past = [None] * len(self.h)\n",
        "        else:\n",
        "            past_length = past[0][0].size(-2)\n",
        "        if position_ids is None:\n",
        "            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length, dtype=torch.long,\n",
        "                                        device=input_ids.device)\n",
        "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "        input_shape = input_ids.size()\n",
        "        input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "        position_ids = position_ids.view(-1, position_ids.size(-1))\n",
        "\n",
        "        inputs_embeds = self.wte(input_ids)\n",
        "        position_embeds = self.wpe(position_ids)\n",
        "        if token_type_ids is not None:\n",
        "            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n",
        "            token_type_embeds = self.wte(token_type_ids)\n",
        "        else:\n",
        "            token_type_embeds = 0\n",
        "        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n",
        "        presents = []\n",
        "        for layer_idx, (block, layer_past) in enumerate(zip(self.h, past)):\n",
        "            hidden_states, present = block(hidden_states, layer_past, name_prefix=f'block{layer_idx}')\n",
        "            presents.append(present)\n",
        "        hidden_states = self.ln_f(hidden_states)\n",
        "        output_shape = input_shape + (hidden_states.size(-1),)\n",
        "        return hidden_states.view(*output_shape), presents\n",
        "\n",
        "class GPT2LMHead(nn.Module):\n",
        "    def __init__(self, model_embeddings_weights, config):\n",
        "        super(GPT2LMHead, self).__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "        self.set_embeddings_weights(model_embeddings_weights)\n",
        "\n",
        "    def set_embeddings_weights(self, model_embeddings_weights):\n",
        "        embed_shape = model_embeddings_weights.shape\n",
        "        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n",
        "        self.decoder.weight = model_embeddings_weights  # Tied weights\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        # Truncated Language modeling logits (we remove the last token)\n",
        "        # h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd)\n",
        "        lm_logits = self.decoder(hidden_state)\n",
        "        return lm_logits\n",
        "\n",
        "class GPT2LMHeadModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GPT2LMHeadModel, self).__init__()\n",
        "        self.transformer = GPT2Model(config)\n",
        "        self.lm_head = GPT2LMHead(self.transformer.wte.weight, config)\n",
        "\n",
        "    def set_tied(self):\n",
        "        \"\"\" Make sure we are sharing the embeddings\n",
        "        \"\"\"\n",
        "        self.lm_head.set_embeddings_weights(self.transformer.wte.weight)\n",
        "\n",
        "    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n",
        "        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n",
        "        lm_logits = self.lm_head(hidden_states)\n",
        "        if lm_labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))\n",
        "            return loss\n",
        "        return lm_logits, presents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDFbKR-GDj42"
      },
      "source": [
        "'''\n",
        "Modified from https://github.com/graykode/gpt-2-Pytorch/blob/master/GPT2/sample.py\n",
        "See above for original license.\n",
        "'''\n",
        "\n",
        "'''\n",
        "    code by TaeHwan Jung(@graykode)\n",
        "    Original Paper and repository here : https://github.com/openai/gpt-2\n",
        "    GPT2 Pytorch Model : https://github.com/huggingface/pytorch-pretrained-BERT\n",
        "'''\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def top_k_logits(logits, k):\n",
        "    if k == 0:\n",
        "        return logits\n",
        "    values, _ = torch.topk(logits, k)\n",
        "    min_values = values[:, -1]\n",
        "    return torch.where(logits < min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits)\n",
        "\n",
        "def sample_sequence(model, length, context, batch_size=None, temperature=1, top_k=0, device='cuda', sample=True):\n",
        "    context = torch.tensor(context, device=device, dtype=torch.long)\n",
        "    prev = context\n",
        "    output = context\n",
        "    past = None\n",
        "    with torch.no_grad():\n",
        "        for i in range(length):\n",
        "            logits, past = model(prev, past=past)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            logits = top_k_logits(logits, k=top_k)\n",
        "            log_probs = F.softmax(logits, dim=-1)\n",
        "            if sample:\n",
        "                prev = torch.multinomial(log_probs, num_samples=1)\n",
        "            else:\n",
        "                _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
        "            output = torch.cat((output, prev), dim=1)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAwTzQz6DoF_"
      },
      "source": [
        "# load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIWFnb-LDmbG"
      },
      "source": [
        "# Download the GPT-2 model weights\n",
        "!curl -o gpt2-model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-pytorch_model.bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp5U9Ss6DzrV"
      },
      "source": [
        "import gc\n",
        "\n",
        "if 'model' in globals():\n",
        "    print('Cleaning up old model')\n",
        "    del model\n",
        "gc.collect()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "enc = get_encoder()\n",
        "config = GPT2Config(\n",
        "    n_positions=1024,\n",
        "    n_ctx=1024,\n",
        "    n_embd=1600,\n",
        "    n_layer=48,\n",
        "    n_head=25,\n",
        ")\n",
        "model = GPT2LMHeadModel(config).to(device)\n",
        "if 'state_dict' not in globals():\n",
        "    state_dict = torch.load('gpt2-model.bin', map_location=device)\n",
        "model = load_weight(model, state_dict)\n",
        "model.eval()\n",
        "print('Loaded weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYX8ZYFXyMob"
      },
      "source": [
        "# load the dataset\n",
        "\n",
        "(uploaded here from the other notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA9UKp6hD22i"
      },
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import random\n",
        "import os\n",
        "\n",
        "assert os.path.exists('dataset.json'), 'Missing dataset.json! Did you upload it from the other notebook?'\n",
        "\n",
        "with open('dataset.json', 'r') as f:\n",
        "    dataset = json.loads(f.read())\n",
        "print('Dataset has', len(dataset), 'examples.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtuCfs192eAQ"
      },
      "source": [
        "# register hooks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ4l2bld2dYa"
      },
      "source": [
        "import pickle\n",
        "import torch\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "# make sure we clear up any existing hooks if this cell is re-run\n",
        "if 'handles' in globals():\n",
        "    print('Removing existing hooks...')\n",
        "    for handle in handles:\n",
        "        handle.remove()\n",
        "\n",
        "module_names_to_track_for_activations = \\\n",
        "    [f'transformer.h.{i}.mlp.c_proj' for i in range(config.n_layer)]\n",
        "module_names_to_track_for_logits = \\\n",
        "    [f'transformer.h.{i}' for i in range(config.n_layer)]\n",
        "\n",
        "hidden_states = dict()\n",
        "def save_hidden_states(name, module, input, output):\n",
        "    # we care about input for activations, and output for logit lens\n",
        "    hidden_states[name] = { 'input': input[0], 'output': output }\n",
        "\n",
        "handles = []\n",
        "cnt = 0\n",
        "for name, m in model.named_modules():\n",
        "    if name in module_names_to_track_for_activations or name in module_names_to_track_for_logits:\n",
        "        cnt += 1\n",
        "        handle = m.register_forward_hook(partial(save_hidden_states, name))\n",
        "        handles.append(handle)\n",
        "\n",
        "print('Tracking', cnt, 'layers.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCa4D7cDNHmT"
      },
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVEt0ufGZYVr"
      },
      "source": [
        "# some settings\n",
        "\n",
        "num_tokens = 100\n",
        "top_k = 5\n",
        "activation_threshold = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qjXrbMREAj4"
      },
      "source": [
        "import jsonlines\n",
        "\n",
        "\n",
        "def run_model_with_tracking(tokens):\n",
        "    with torch.no_grad():\n",
        "        context = torch.tensor([tokens], device=device, dtype=torch.long)\n",
        "        # effectively:\n",
        "        #  length = 1\n",
        "        #  temp = 0\n",
        "        #  batch_size = 1\n",
        "        logits, past = model(context, past=None)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def extract_neuron_values(threshold=5):\n",
        "    \"\"\"\n",
        "    For each MLP, determine which neurons fire at any point during the entire sequence,\n",
        "    unless it only fires on the first token (which we will just assume is noise).\n",
        "\n",
        "    The output is a list of dicts resembling individual neurons with fields:\n",
        "        l: the layer of the neuron\n",
        "        f: the index of the neuron in the feature dimension\n",
        "        a: a list of activations equal to the length of the sequence\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    neurons = []\n",
        "    for name in module_names_to_track_for_activations:\n",
        "        h = hidden_states[name]['input']\n",
        "        neurons.append(h[0])\n",
        "    neurons = torch.stack(neurons)\n",
        "    # ignore first activations (too noisy!)\n",
        "    high_activations = (neurons[:, 1:, :] > threshold).nonzero()\n",
        "    values = []\n",
        "    uniq = set()\n",
        "    for layer_idx, _, feature_idx in high_activations:\n",
        "        layer_idx = layer_idx.item()\n",
        "        feature_idx = feature_idx.item()\n",
        "        if (layer_idx, feature_idx) in uniq:\n",
        "            # we already have it!\n",
        "            continue\n",
        "        uniq.add((layer_idx, feature_idx))\n",
        "        values.append({\n",
        "            'l': layer_idx,\n",
        "            'f': feature_idx,\n",
        "            'a': neurons[layer_idx, :, feature_idx].reshape([neurons.shape[1]]).tolist(),\n",
        "        })\n",
        "    return values\n",
        "\n",
        "\n",
        "def extract_logit_lens(k=10):\n",
        "    \"\"\"\n",
        "    Extract the output logits for each layer (including the final layer)\n",
        "\n",
        "    Returns a nested list structure of shape [n_layers, n_seq, k]\n",
        "    where each element is a dict containing:\n",
        "        tok: the predicted token\n",
        "        prob: the probability given to this token (from softmax of logits)\n",
        "\n",
        "    Note: The sum of the final dimension probabilities will be very close to 1.\n",
        "    \"\"\"\n",
        "\n",
        "    per_layer_tokens = []\n",
        "    for name in module_names_to_track_for_logits:\n",
        "        h2 = hidden_states[name]['output'][0]  # x, present\n",
        "        with torch.no_grad():\n",
        "            layer_logits = model.lm_head(h2).detach()[0]\n",
        "        seq = layer_logits.shape[0]\n",
        "        values, indices = torch.topk(layer_logits, k=k)\n",
        "        norm_values = F.softmax(values, dim=-1)\n",
        "        indices = indices.cpu()\n",
        "        norm_values = norm_values.cpu()\n",
        "        top_in_sequence = []\n",
        "        for i in range(seq):\n",
        "            top_tokens = []\n",
        "            for tok, prob in zip(indices[i], norm_values[i]):\n",
        "                tok = tok.item()\n",
        "                prob = prob.item()\n",
        "                top_tokens.append({\n",
        "                    'tok': enc.decode([tok]),\n",
        "                    'prob': prob,\n",
        "                })\n",
        "            top_in_sequence.append(top_tokens)\n",
        "        per_layer_tokens.append(top_in_sequence)\n",
        "    return per_layer_tokens\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    with jsonlines.open('neurons.jsonl', 'w') as writer:\n",
        "        for row in tqdm(dataset):\n",
        "            text = row['text']\n",
        "            tokens = enc.encode(text)\n",
        "            tokens = tokens[:num_tokens]\n",
        "            run_model_with_tracking(tokens)\n",
        "            activations = extract_neuron_values(threshold=activation_threshold)\n",
        "            logits = extract_logit_lens(k=top_k)  # [48, seq, 5]\n",
        "            record = {\n",
        "                'activations': activations,\n",
        "                'logits': logits,\n",
        "            }\n",
        "            writer.write(record)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKjVghZTZFJV"
      },
      "source": [
        "# upload the neuron file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQI67ZvSIo1d"
      },
      "source": [
        "!pip install awscli"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4N6I2nuIrMu"
      },
      "source": [
        "!aws configure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJWp-jcZ9zp-"
      },
      "source": [
        "!aws s3 cp neurons.jsonl s3://gpt2-neurons/wikipedia-first-lines/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLF8K8udeROL"
      },
      "source": [
        "# cluster the neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc5dMSjpezj7"
      },
      "source": [
        "from collections import defaultdict\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import jsonlines\n",
        "import torch\n",
        "\n",
        "\n",
        "def iter_neuron_records():\n",
        "    with jsonlines.open(f'neurons.jsonl') as reader:\n",
        "        for obj in reader:\n",
        "            yield obj\n",
        "\n",
        "def iter_neurons():\n",
        "    for l in range(48):\n",
        "        for f in range(1600*4):\n",
        "            yield l, f\n",
        "\n",
        "def first_n(gen, n):\n",
        "    buffer = []\n",
        "    for v, idx in zip(gen, range(n)):\n",
        "        buffer.append(v)\n",
        "    return buffer\n",
        "\n",
        "def normalize(v):\n",
        "    v = [max(0, e) for e in v]\n",
        "    mx = max(v)\n",
        "    v = [e / mx for e in v]\n",
        "    return v\n",
        "\n",
        "# essentially, for each neuron, take the top N\n",
        "# examples that this neuron responds to.\n",
        "# Then, for each, multiply the weighted sum of all embedded tokens\n",
        "# in the sequenced, weighted by the normalized activations,\n",
        "# then concat all of these.\n",
        "\n",
        "neuron_to_example_indices = defaultdict(list)\n",
        "neuron_records = iter_neuron_records()\n",
        "num_examples = len(dataset)\n",
        "for idx in tqdm(range(num_examples)):\n",
        "    example = dataset[idx]\n",
        "    neuron_record = next(neuron_records)\n",
        "    for record in neuron_record['activations']:\n",
        "        mx = max(record['a'])\n",
        "        if mx >= 2:\n",
        "            neuron_to_example_indices[(record['l'], record['f'])].append({ 'activations': record['a'], 'exampleIdx': idx })\n",
        "neurons = []\n",
        "for l, f in iter_neurons():\n",
        "    if (l, f) in neuron_to_example_indices:\n",
        "        neurons.append((l, f))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-SsY7iNoc6I"
      },
      "source": [
        "def embed_tokens(tokens):\n",
        "    with torch.no_grad():\n",
        "        input_ids = torch.tensor([tokens], device=device, dtype=torch.long)\n",
        "\n",
        "        position_ids = torch.arange(0, input_ids.size(-1), dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "        input_shape = input_ids.size()\n",
        "        input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "        position_ids = position_ids.view(-1, position_ids.size(-1))\n",
        "\n",
        "        inputs_embeds = model.transformer.wte(input_ids)\n",
        "        position_embeds = model.transformer.wpe(position_ids)\n",
        "        hidden_states = inputs_embeds + position_embeds\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "latent_vectors = []\n",
        "for l, f in tqdm(neurons):\n",
        "    top_examples = first_n(\n",
        "        sorted(\n",
        "            neuron_to_example_indices[(l, f)],\n",
        "            key=lambda x: max(x['activations'])\n",
        "        ),\n",
        "        1\n",
        "    )\n",
        "    latent_vector_2d = []\n",
        "    for top_example in top_examples:\n",
        "        idx = top_example['exampleIdx']\n",
        "        normalized_activations = torch.tensor(normalize(top_example['activations']))\n",
        "        l = normalized_activations.shape[0]\n",
        "        normalized_activations = normalized_activations.reshape((l, 1))\n",
        "        example = dataset[idx]\n",
        "        tokens = enc.encode(example['text'])\n",
        "        embedded_tokens = embed_tokens(tokens[:l]).detach().cpu()[0]\n",
        "        latent_vector_part = torch.multiply(embedded_tokens, normalized_activations).sum(dim=0)\n",
        "        latent_vector_2d.append(latent_vector_part)\n",
        "    latent_vector = latent_vector_2d[0]\n",
        "    latent_vectors.append(latent_vector)\n",
        "latent_vectors = torch.stack(latent_vectors)\n",
        "latent_vectors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyYooM_xumVR"
      },
      "source": [
        "# first compress down to 50 dimensions with PCA,\n",
        "# then down to 2 with t-SME\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=100, whiten=True)\n",
        "pca.fit(latent_vectors.T)\n",
        "pca.components_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXytnJNrletT"
      },
      "source": [
        "!pip install opentsne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzhsqbnHlZE4"
      },
      "source": [
        "from openTSNE import TSNE\n",
        "import time\n",
        "\n",
        "tsne = TSNE(\n",
        "    perplexity=30,\n",
        "    metric=\"euclidean\",\n",
        "    n_jobs=2,\n",
        "    random_state=42,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# use about 30% of the neurons to building an embedding in 2D:\n",
        "st = time.time()\n",
        "X = np.array(random.sample(list(pca.components_.T), 30000))\n",
        "embedding = tsne.fit(X)\n",
        "X = embedding.transform(pca.components_.T).T\n",
        "en = time.time()\n",
        "print(en-st)\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIfGoDxxNpQQ"
      },
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "hovertext = [str(neuron) for neuron in neurons]\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=X[0],\n",
        "    y=X[1],\n",
        "    hovertext=hovertext,\n",
        "    hoverinfo=\"text\",\n",
        "    showlegend=False,\n",
        "    mode='markers',\n",
        "    marker={\n",
        "        'size': 2,\n",
        "    },\n",
        "))\n",
        "\n",
        "fig.show()\n",
        "# px.scatter(x=X[0], y=X[1], hovertext=[str(neuron) for neuron in neurons])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8OUjmS_Xsvs"
      },
      "source": [
        "# create the index for fast viewing on the browser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA-qWBjdXsCG"
      },
      "source": [
        "from collections import defaultdict\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import jsonlines\n",
        "\n",
        "\n",
        "def iter_neuron_records():\n",
        "    with jsonlines.open(f'neurons.jsonl') as reader:\n",
        "        for obj in reader:\n",
        "            yield obj\n",
        "\n",
        "def iter_neurons():\n",
        "    for l in range(48):\n",
        "        for f in range(1600*4):\n",
        "            yield l, f\n",
        "\n",
        "\n",
        "base_path = 'index'\n",
        "\n",
        "if not os.path.exists(base_path):\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "num_examples = len(dataset)\n",
        "\n",
        "print(f'Generating example-level indices into the {base_path} folder...')\n",
        "neuron_records = iter_neuron_records()\n",
        "for idx in tqdm(range(num_examples)):\n",
        "    example = dataset[idx]\n",
        "    neuron_record = next(neuron_records)\n",
        "    with open(os.path.join(base_path, f'example-{idx:05}.json'), 'w') as f:\n",
        "        f.write(json.dumps({\n",
        "            'example': example,\n",
        "            'activations': neuron_record['activations'],\n",
        "            'logits': neuron_record['logits'],\n",
        "            'tokens': [enc.decode([t]) for t in enc.encode(example['text'])]\n",
        "        }))\n",
        "\n",
        "print(f'Generating neuron-level indices into the {base_path} folder...')\n",
        "neuron_to_example_indices = defaultdict(list)\n",
        "neuron_records = iter_neuron_records()\n",
        "for idx in tqdm(range(num_examples)):\n",
        "    example = dataset[idx]\n",
        "    neuron_record = next(neuron_records)\n",
        "    for record in neuron_record['activations']:\n",
        "        mx = max(record['a'])\n",
        "        if mx >= 2:\n",
        "            neuron_to_example_indices[(record['l'], record['f'])].append({ 'activations': record['a'], 'exampleIdx': idx })\n",
        "\n",
        "for k in neuron_to_example_indices.keys():\n",
        "    neuron_to_example_indices[k] = list(sorted(neuron_to_example_indices[k], key=lambda e: max(e['activations']), reverse=True))\n",
        "\n",
        "for (l, f) in iter_neurons():\n",
        "    if (l, f) in neuron_to_example_indices:\n",
        "        with open(os.path.join(base_path, f'neuron-{l}-{f}.json'), 'w') as file:\n",
        "            file.write(json.dumps(neuron_to_example_indices[(l, f)]))\n",
        "\n",
        "print(f'Generating neuron-cluster indices into the {base_path} folder...')\n",
        "with open(os.path.join(base_path, 'cluster.json'), 'w') as fp:\n",
        "    obj = {\n",
        "        'x': [round(e, 3) for e in X[0]],\n",
        "        'y': [round(e, 3) for e in X[1]],\n",
        "        'neurons': neurons,\n",
        "    }\n",
        "    json.dump(obj, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxaoeVLVvt40"
      },
      "source": [
        "!du -h {base_path}/cluster.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTLnfi96YYmv"
      },
      "source": [
        "# zip the files\n",
        "!zip -rq index.zip ./index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG_r7JL_bFo8"
      },
      "source": [
        "!aws s3 cp index.zip s3://gpt2-neurons/wikipedia-first-lines/index2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIZ7AoWyYmkB"
      },
      "source": [
        "# (you can stop here, the rest is just computing statistics on the data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkPH9dh2N_Jt"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "unique_neurons = set()\n",
        "for record in records:\n",
        "    for neuron in record['activations']:\n",
        "        unique_neurons.add((neuron['l'], neuron['f']))\n",
        "print(len(unique_neurons))\n",
        "\n",
        "per_layer_totals = defaultdict(lambda: [])\n",
        "for record in records:\n",
        "    per_layer_counts = defaultdict(lambda: 0)\n",
        "    for neuron in record['activations']:\n",
        "        l = neuron['l']\n",
        "        per_layer_counts[l] += 1\n",
        "    for k, v in per_layer_counts.items():\n",
        "        per_layer_totals[k].append(v)\n",
        "for k, v in sorted(per_layer_totals.items()):\n",
        "    print(k, ':', sum(v) / len(v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSoykQQELNL3"
      },
      "source": [
        "with open('neurons.json', 'w') as f:\n",
        "    f.write(json.dumps(records))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2USkAtlfIWrE"
      },
      "source": [
        "!du -h neurons.json"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}